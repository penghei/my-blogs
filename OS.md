---
title: 操作系统知识点总结
date: 2022-05-12 15:58:22
tags: 日常学习
categories: 原理
cover:
---

# 进程和线程

## 进程出现的原因

程序不能进行并发，但操作系统需要一次执行多个任务来提升效率。
进程有着比程序更高的灵活性和可控性。比如进程可以被挂起（阻塞），cpu 可以通过调度多个进程的先后执行来使得不同任务并发执行，而这对于程序来说是不可能的，很难在执行一段程序期间中断去执行另一段

## 进程

### 进程的定义

1. 进程是程序的一次执行。
2. 进程是一个程序及其数据在处理机上顺序执行时所发生的活动。
3. 进程是程序在一个数据集合上运行的过程，它是**系统进行资源分配和调度的一个独立单位**。

我们编写的代码只是一个存储在硬盘的静态文件，通过编译后就会生成二进制可执行文件，当我们运行这个可执行文件后，它会被装载到内存中，接着 CPU 会执行程序中的每一条指令，那么这个运行中的程序，就被称为「进程」（Process）。

### 进程的组成

为使程序（含数据）能独立运行，应为之配置一个进程控制块，即 PCB(Process Control Block)；

由程序段、数据段和 PCB 三部分便构成了进程实体。

- 程序段：即进程中包含的代码部分
- 数据段：进程存储的数据资源
- PCB：进程控制块，负责进程的管理、标识、调度

#### PCB

PCB 是进程存在的唯一标识，这意味着一个进程的存在，必然会有一个 PCB，如果进程消失了，那么 PCB 也会随之消失。

PCB 主要包含的信息：

- 进程标识符：标识各个进程，每个进程都有一个并且唯一的标识符；
- 用户标识符：进程归属的用户，用户标识符主要为共享和保护服务；
- 进程当前状态，如 new、ready、running、waiting 或 blocked 等；
- 进程优先级：进程抢占 CPU 时的优先级；
- 资源分配清单：有关内存地址空间或虚拟地址空间的信息，所打开文件的列表和所使用的 I/O 设备信息。
- CPU 相关信息：CPU 中各个寄存器的值，当进程被切换时，CPU 的状态信息都会被保存在相应的 PCB 中，以便进程重新执行时，能从断点处继续执行。

PCB 是通常是通过链表的方式进行组织，把具有相同状态的进程链在一起，组成各种队列。比如：

- 将所有处于就绪状态的进程链在一起，称为就绪队列；
- 把所有因等待某事件而处于等待状态的进程链在一起就组成各种阻塞队列；

另外，对于运行队列在单核 CPU 系统中则只有一个运行指针了，因为单核 CPU 在某个时间，只能运行一个程序。
因为可能面临进程创建，销毁等调度导致进程状态发生变化，链表能够更加灵活的插入和删除。

### 进程的特点

1. 动态性：进程的实质是进程实体的一次执行过程，因此，动态性是进程的**最基本的特征**。 动态性表现为：由创建而产生，由调度而执行，由撤消而消亡。可见，进程实体有一定的生命期。
2. 并发性：多个进程实体同存于内存中，且能在一段时间内同时运行。

> 并发和并行的区别：
> 并发是某一刻只有一个任务在进行，但是某个很短的时间段内任务交替执行，看起来就像是在并行
> 并行是真正的并发执行，需要多核处理器

3. 独立性：指进程实体是一个能独立运行、独立分配资源和独立接受调度的基本单位；
4. 异步性：指进程按各自独立的、不可预知的速度向前推进，或说进程实体按异步方式运行。

### 进程的状态

#### 三种基本状态

![](https://pic.imgdb.cn/item/627ccbcf0947543129f6e146.jpg)

- 就绪状态（Ready）：当进程已分配到除 CPU 以外的所有必要资源后，只要再获得 CPU，便可立即执行。
- 执行状态(Running)：进程已获得 CPU，其程序正在执行。
- 阻塞状态(Blocked)：正在执行的进程由于发生某事件而暂时无法继续执行时，当前进程被暂停，cpu 转去执行其他进程或其他任务，把这种暂停状态称为阻塞状态，有时也称为等待状态。

#### 五种完整状态

其实就是在三种基本状态上引入创建和结束状态

![](https://pic1.imgdb.cn/item/63626d8d16f2c2beb114fec7.jpg)

- NULL -> 创建状态：一个新进程被创建时的第一个状态；
- 创建状态 -> 就绪状态：当进程被创建完成并初始化后，一切就绪准备运行时，变为就绪状态，这个过程是很快的；
- 运行状态 -> 结束状态：当进程已经运行完成或出错时，会被操作系统作结束状态处理；

#### 引入挂起状态

如果有大量处于阻塞状态的进程，进程可能会占用着物理内存空间，所以在虚拟内存管理的操作系统中，通常会把阻塞状态的进程的物理内存空间换出到硬盘，等需要再次运行的时候，再从硬盘换入到物理内存。
**进程没有占用实际物理内存的状态，就叫做挂起状态**。

挂起状态可以分为两种：

- 阻塞挂起状态：进程在外存（硬盘）并等待某个事件的出现；
- 就绪挂起状态：进程在外存（硬盘），但只要进入内存，即刻立刻运行；

![](https://pic1.imgdb.cn/item/63626e1516f2c2beb115db7a.jpg)

进入挂起状态不一定是系统的调配，还可以是用户的控制。比如调用 sleep 让进程挂起。

虽然挂起状态和阻塞状态都是表示进程在等待某些任务而暂停，但挂起状态下进程不能直接转换为就绪状态，需要将其分配实际的物理内存才能进入三个基本状态的循环。

### 进程的控制

#### 状态改变

进程的创建、终止、阻塞、唤醒的过程，这些过程也就是进程的控制。
进程控制的核心，除了分配内存、改变 PCB 状态之外，还有一个核心操作就是将 PCB 插入或从某个队列中删除。比如创建进程就是插入到就绪队列，阻塞进程就是插入到阻塞队列。

1. 创建进程

操作系统允许一个进程创建另一个进程，而且允许子进程继承父进程所拥有的资源。

创建进程的过程如下：

- 申请一个空白的 PCB，并向 PCB 中填写一些控制和管理进程的信息，比如进程的唯一标识等；
- 为该进程分配运行时所必需的资源，比如内存资源；
- 将 PCB 插入到就绪队列，等待被调度运行；

2. 终止进程

进程可以有 3 种终止方式：正常结束、异常结束以及外界干预（信号 kill 掉）。

当子进程被终止时，其在父进程处继承的资源应当还给父进程。而当父进程被终止时，该父进程的子进程就变为孤儿进程，会被 1 号进程收养，并由 1 号进程对它们完成状态收集工作。

终止进程的过程如下：

- 查找需要终止的进程的 PCB；
- 如果处于执行状态，则立即终止该进程的执行，然后将 CPU 资源分配给其他进程；
- 如果其还有子进程，则应将该进程的子进程交给 1 号进程接管；
- 将该进程所拥有的全部资源都归还给操作系统；
- 将其从 PCB 所在队列中删除；

3. 阻塞进程

当进程需要等待某一事件完成时，它可以调用阻塞语句把自己阻塞等待。而一旦被阻塞等待，它只能由另一个进程唤醒。

阻塞进程的过程如下：

- 找到将要被阻塞进程标识号对应的 PCB；
- 如果该进程为运行状态，则保护其现场，将其状态转为阻塞状态，停止运行；
- 将该 PCB 插入到阻塞队列中去；

4. 唤醒进程

进程由「运行」转变为「阻塞」状态是由于进程必须等待某一事件的完成，所以处于阻塞状态的进程是绝对不可能叫醒自己的。

如果某进程正在等待 I/O 事件，需由别的进程发消息给它，则只有当该进程所期待的事件出现时，才由发现者进程用唤醒语句叫醒它。

唤醒进程的过程如下：

- 在该事件的阻塞队列中找到相应进程的 PCB；
- 将其从阻塞队列中移出，并置其状态为就绪状态；
- 把该 PCB 插入到就绪队列中，等待调度程序调度；
- 进程的阻塞和唤醒是一对功能相反的语句，如果某个进程调用了阻塞语句，则必有一个与之对应的唤醒语句

### 进程的调度

在进程的生命周期中，当进程从一个运行状态到另外一状态变化的时候，其实会触发一次调度。

根据如何处理时钟中断 ，把调度算法分为两类：

- 非抢占式调度：算法挑选一个进程，然后让该进程运行直到被阻塞，或者直到该进程退出，才会调用另外一个进程，也就是说不会理时钟中断这个事情。
- 抢占式调度：算法挑选一个进程，然后让该进程只运行某段时间，如果在该时段结束时，该进程仍然在运行时，则会把它挂起，接着调度程序从就绪队列挑选另外一个进程。这种抢占式调度处理，需要在时间间隔的末端发生时钟中断，以便把 CPU 控制返回给调度程序进行调度，也就是常说的时间片机制。

#### 调度的原则

原则一：如果运行的程序，发生了 I/O 事件的请求，那 CPU 使用率必然会很低，因为此时进程在阻塞等待硬盘的数据返回。这样的过程，势必会造成 CPU 突然的空闲。所以，为了提高 CPU 利用率，在这种发送 I/O 事件致使 CPU 空闲的情况下，调度程序需要从就绪队列中选择一个进程来运行。

原则二：有的程序执行某个任务花费的时间会比较长，如果这个程序一直占用着 CPU，会造成系统吞吐量（CPU 在单位时间内完成的进程数量）的降低。所以，要提高系统的吞吐率，调度程序要权衡长任务和短任务进程的运行完成数量。

原则三：从进程开始到结束的过程中，实际上是包含两个时间，分别是进程运行时间和进程等待时间，这两个时间总和就称为周转时间。进程的周转时间越小越好，如果进程的等待时间很长而运行时间很短，那周转时间就很长，这不是我们所期望的，调度程序应该避免这种情况发生。

原则四：处于就绪队列的进程，也不能等太久，当然希望这个等待的时间越短越好，这样可以使得进程更快的在 CPU 中执行。所以，就绪队列中进程的等待时间也是调度程序所需要考虑的原则。

原则五：对于鼠标、键盘这种交互式比较强的应用，我们当然希望它的响应时间越快越好，否则就会影响用户体验了。所以，对于交互式比较强的应用，响应时间也是调度程序需要考虑的原则。

总结调度要注意的核心，如下：

- CPU 利用率：调度程序应确保 CPU 是始终匆忙的状态，这可提高 CPU 的利用率；
- 系统吞吐量：吞吐量表示的是单位时间内 CPU 完成进程的数量，长作业的进程会占用较长的 CPU 资源，因此会降低吞吐量，相反，短作业的进程会提升系统吞吐量；
- 周转时间：周转时间是进程运行+阻塞时间+等待时间的总和，一个进程的周转时间越小越好；
- 等待时间：这个等待时间不是阻塞状态的时间，而是进程处于就绪队列的时间，等待的时间越长，用户越不满意；
- 响应时间：用户提交请求到系统第一次产生响应所花费的时间，在交互式系统中，响应时间是衡量调度算法好坏的主要标准。

#### 调度算法

调度算法和实际场景相关，在不同的 cpu、不同的操作系统下，采取的调度算法也不同。

比如 cpu 分为单核、多核，操作系统也分为分时、实时，不同的结构下采取的算法也不相同。

常见的调度算法：

1. 单核 CPU 调度算法
   1. 先来先服务调度算法 FCFS：每次从就绪队列选择最先进入队列的进程，然后一直运行，直到进程退出或被阻塞，才会继续从队列中选择第一个进程接着运行。
   2. 最短作业优先调度算法 SJF：优先选择运行时间最短的进程来运行，这有助于提高系统的吞吐量。
   3. 高响应比优先调度算法 HRRN：每次进行进程调度时，先计算「响应比优先级」，然后把「响应比优先级」最高的进程投入运行。
      ![](https://pic.imgdb.cn/item/641eaa94a682492fcc8a0cbf.jpg)
      高响应比优先调度算法是「理想型」的调度算法，现实中是实现不了的。因为一个进程要求服务的时间（即预测要多久才能执行完这个进程）是不可能在运行前知道的。
   4. 时间片轮转调度算法 RR：每个进程被分配一个时间段，称为时间片（Quantum），即允许该进程在该时间段中运行。如果时间片用完，进程还在运行，那么将会把此进程从 CPU 释放出来，并把 CPU 分配给另外一个进程；如果该进程在时间片结束前阻塞或结束，则 CPU 立即进行切换；
   5. 最高优先级调度算法 HPF：引入了优先级的概念，每次选择优先级最高的先运行，优先级有动态（确定就不再改变）和静态之分，该算法也有抢占和非抢占两种
   6. 多级反馈队列调度算法 MFQ：是「时间片轮转算法」和「最高优先级算法」的综合和发展。即，设定多级的队列，从上到下队列中每个任务的时间片逐渐增加，优先级逐渐降低。如果一个任务在某个队列中的限定的时间片内未执行完，就会被放到下一个队列去执行。优先级高的任务会先执行。
      第一级队列内的任务按照 FCFS 执行，如果该任务在限定的时间片内没运行完，就会插入到下一级队列的后面。依次类推，直到最后一级，任务一定会被执行。
      ![](https://pic.imgdb.cn/item/641eab77a682492fcc8b635a.jpg)

### 进程通信

进程通信的手段：

1. 管道：管道传输数据是单向的，类似于一个 FIFO 队列，当数据量非常大时效率很低，不适合进程间频繁地交换数据

管道本质上是内核里面的一串缓存。从管道的一段写入的数据，实际上是缓存在内核中的，另一端读取，也就是从内核中读取这段数据。另外，管道传输的数据是无格式的流且大小受限。

管道分为命名管道和匿名管道。
- 匿名管道：只支持单向传输，用完即销毁，通常是在父子进程之间通信
- 命名管道：可以在不相关的进程之间通信。当输入数据时，需要另一端取出数据才能继续

管道随着内核产生和销毁而产生和销毁

2. 消息队列：消息队列是保存在内核中的消息链表，基本单位是“消息”，是用户自定义的数据类型，发送方和接收方要提前约定好消息的类型、大小等。每个消息体都是固定大小的存储块。消息队列机制不足的地方在于通信有延时，并且传输的数据量不能太大



3. 共享内存：共享内存的机制，就是拿出一块虚拟地址空间来，映射到相同的物理内存中；这样这个进程写入的东西，另外一个进程马上就能看到。



4. 信号量：本质上是一个计数器，常用于实现进程的同步，并非缓存数据。信号量常见的操作是 P/V 操作，即减少/增加信号量，相当于申请/释放资源

常见的信号量分为同步信号量（初始化为0）和互斥信号量（初始化为1）

5. 信号：表示进程之间发送一种信号。不等同于信号量，而是直接由某个进程向另一个进程发送，另一个进程接收后执行约定好的操作。信号机制是通信中唯一的异步机制

信号常用于进程异常的通信，比如杀死进程、挂起进程等等

6. socket：主机之间的通信手段，即 TCP 和 UDP 采用的方式。

### 进程同步

为了使进程之间能更好地相互合作，操作系统引入了两种主要的进程间关系：互斥和同步

- 互斥：指多个进程访问同一个临界资源时，一次只能有一个进程访问该资源，当有一个进程访问时排斥掉其他任何进程的访问。也就说保证一个进程在临界区执行时，其他进程应该被阻止进入临界区
- 同步：并发进程/线程在一些关键点上可能需要互相等待与互通消息，这种相互制约的等待与互通信息称为进程/线程同步。

理解一下就是：
- 互斥：进程之间不能同时访问一个资源
- 同步：进程之间应该按顺序访问一个资源

进程同步的主要任务：是使并发执行的诸进程之间能有效地共享资源和相互合作，从而使程序的执行具有可再现性。

同步可以理解为调整不同进程之间的执行顺序，使他们能更好地共享资源和共同协作。注意这是两点

同步机制应遵循的规则

1. 空闲让进
2. 忙则等待
3. 有限等待
4. 让权等待

#### 同步的方式

进程的同步主要可以通过两种方式实现：

1. 同步锁机制，即一个进程访问临界资源时申请到一个 🔒，必须持有这个 🔒 才能访问，因此会排斥其他的线程。
   🔒 的形式有两种：
   - 一种是忙等待 🔒，即当进程申请不到锁时持续通过 while 循环阻塞，直到申请到锁为止；
   - 一种是非忙等待，即当进程申请锁申请不到时，将该进程放入改资源的等待队列中，当该进程的锁被释放，就取出等待队列的进程执行。
2. 信号量（sem）机制，即信号量表示资源的数量，通过 PV 操作控制。每个进程访问临界资源时必须先通过 P 操作减少一个信号量，访问完毕时再通过 V 操作释放信号量；当信号量<0 时就必须等待信号量释放才可以进入。

- P 操作：将 sem 减 1，相减后，如果 sem < 0，则进程/线程进入阻塞等待，否则继续，表明 P 操作可能会阻塞；
- V 操作：将 sem 加 1，相加后，如果 sem <= 0，唤醒一个等待中的进程/线程，表明 V 操作不会阻塞；

P 操作和 V 操作可以看做是几个操作的集合，即一个原子操作。P 操作通常是对信号量维持一个“监听”状态，当信号量不足时 P 会一直等待。如果信号量足够，那么就会调度进程运行。
举个例子：

```c
semaphore s1 = 0;
semaphore s2 = 0;
void son()
{
    while(TURE)
    {
        一些操作
        P(s1)
        V(s2)
        一些操作
    }
}
```

这个例子里，s1 初始为 0，那么 P(s1)操作执行后 s1 变为-1，此时暂时不能执行，就进入等待状态。
一旦执行了 V(s1)操作，s1 变为 0，这时 P(s1)就会停止阻塞，也就可以继续向下运行了。对 s2 也是同理。

#### 基本概念

1. 临界资源：一次仅允许一个进程访问的资源为临界资源。
2. 临界区：把在每个进程中访问临界资源的那段代码称为临界区。
3. 信号量机制；信号量即一类临界资源，用于表示资源的数目，通常只能通过 wait 和 signal 操作进行减少和增加

#### 生产者消费者问题

基本原理：通过信号量实现互斥。需要三个信号量，分别是：

- 互斥信号量 mutex：用于互斥访问缓冲区，初始化值为 1；
- 资源信号量 full：用于消费者询问缓冲区是否有数据，有数据则读取数据，初始化值为 0（表明缓冲区一开始为空）；
- 资源信号量 empty：用于生产者询问缓冲区是否有空位，有空位则生成数据，初始化值为 n （缓冲区大小）；

伪代码如下：

```C
semaphore mutex=1; // 用于互斥访问缓冲区，每个线程要访问都必须先申请，否则不能访问
semaphore full=0; // 表示满位的数量（已经放入的资源的数量）
semaphore empty=N; // 表示空位的数量（初始为缓冲区的大小）

void producer( ) {
    while (1) {
        wait(empty) // 将空位数量-1，如果没有空位就不会执行
        wait(mutex)
        // 生产一个数据
        signal(mutex);
        signal(full) // 将满位数量+1
    }
}

void consumer ( ) {
    while (1) {
        wait(full) // 将满位数量-1
        wait(mutex);
        // 取走一个数据
        signal(mutex);
        signal(empty) // 将空位数量+1
    }
}

void main(){
    cobegin // 同时启动两个进程
    producer(); consumer();
    coend
}
```

#### 哲学家进餐问题

![](https://pic.imgdb.cn/item/627cd07709475431290594ce.jpg)

问题描述：五个哲学家坐在一张桌子旁，做以下两件事情之一：吃饭，或者思考。吃东西的时候，他们就停止思考，思考的时候也停止吃东西。餐桌上有五碗意大利面，每位哲学家之间各有一只餐叉，假设哲学家必须用两只餐叉吃东西。他们只能使用自己左右手边的那两只餐叉。
问题在于如何设计一套规则，使得在哲学家们在完全不交谈，也就是无法知道其他人可能在什么时候要吃饭或者思考的情况下，可以在这两种状态下永远交替下去。

哲学家进餐问题的关键是，如果每个哲学家都拿起自一边的叉子，那么每个人都会等待另一边的叉子，从而无限等待下去，就造成了死锁。

方案一：规定奇数号的哲学家先拿左边的，再拿右边的；偶数号的哲学家相反。当然反过来也可以，目的就是保证相邻的哲学家不会同时申请左边或申请右边。
如果一个奇数位的哲学家没有拿起左边的叉子，那他也不会拿起右边的，而是阻塞在第一步直到拿起左边的叉子为止；
如果这个奇数位的哲学家拿起了左边的叉子，那他就会等待右边的叉子，并且它现在是在“持有左边叉子”的状态，其他人不能用它左边的叉子。

在这种情况下，5 个人的哲学家只能有最多两个人同时用餐。

![](https://pic1.imgdb.cn/item/6367413516f2c2beb1d648f3.jpg)

![](https://pic1.imgdb.cn/item/63673f5f16f2c2beb1d365cf.jpg)

方案二：需要一种 S 操作，这种操作不同于普通的 PV，它可以同时考虑两个信号量，当两个信号量都满足时才会继续。
因此可以规定哲学家只有同时获取到自己左边和右边的叉子才能用餐：

```C
semaphore chopstick[5]={1,1,1,1,1};
do{
    // think;
    Sswait(chopstick[(i+1) % 5],chopstick[i]);
    // eat；
    Ssignal(chopstick[(i+1) % 5],chopstick[i]);
}while(TRUE);
```

## 线程

### 线程的定义

线程的出现是为了减少程序并发执行时的时空开销，让 os 具有更好的并发性。

由于进程还包括了程序运行时的大量资源，因此进程频繁的调度、更替将会导致很大的开销；因此多线程 OS 中将拥有资源的基本单位和分派调度的基本单位分开，**进程只是拥有资源的基本单位，而不是可执行的实体；每一个线程都是一个可执行的实体。CPU 的基本调度单位是线程**。进程处于执行状态实际上是说进程中的线程在执行。

### 线程的特点

1. 调度的基本单位
2. 并发性：一个进程中的多个线程可以并发执行
3. 本身几乎不保存资源，只保存少量自身运行必须的资源，比如寄存器、栈等
4. 多个线程可以共享同一个进程中的全部资源，因此每个线程之间的独立性比较差

### 线程的组成

- 程序段，相当于进程的程序段
- 线程控制块 TCB，保存线程执行的运行状态、优先级、标识等信息
- 少量运行所必须的资源

### 线程的实现

主要有三种线程的实现方式：

- **用户线程（\*User Thread\*）**：在用户空间实现的线程，不是由内核管理的线程，是由用户态的线程库来完成线程的管理；
- **内核线程（\*Kernel Thread\*）**：在内核中实现的线程，是由内核管理的线程；
- **轻量级进程（\*LightWeight Process\*）**：在内核中来支持用户线程；

用户线程虽然是由用户自己创建的，但是如果想要像真的线程一样发挥作用，还是需要和内核线程建立联系。具体的联系有一对多、多对一、多对多三种。

#### 用户线程

用户线程是基于用户态的线程管理库来实现的，那么 TCB 也是在库里面来实现的，对于操作系统而言是看不到这个 TCB 的，它只能看到整个进程的 PCB。

所以，**用户线程的整个线程管理和调度，操作系统是不直接参与的，而是由用户级线程库函数来完成线程的管理，包括线程的创建、终止、同步和调度等。**

用户线程是多对一的关系，即多个用户线程对应一个内核线程。

![用户级线程模型](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/20-%E7%BA%BF%E7%A8%8BPCB-%E4%B8%80%E5%AF%B9%E5%A4%9A%E5%85%B3%E7%B3%BB.jpg)

优点：

1. 方便，轻量，上下文切换速度快，因为是由用户的线程库函数执行创建、调度等操作
2. 拥有可见的 TCB，方便管理和控制，甚至可以在不支持线程的操作系统上创建线程。

缺点：

1. 虽然切换和调度快，但是执行比较慢，每个线程得到的时间片较少
2. 线程启动后不能中断，一个线程阻塞会导致所有线程阻塞。

这种多对一的模式都会有第二个问题，即一个线程出问题，其他线程也会受影响。因为本质上这些多对一的线程都是在一个内核线程上实现并发的，因此一个线程阻塞，其他线程也不能执行，甚至有可能出现全部崩溃。

#### 内核线程

内核线程是由操作系统管理的，线程对应的 TCB 自然是放在操作系统里的，这样线程的创建、终止和管理都是由操作系统负责。

内核线程的模型，也就类似前面提到的**一对一**的关系，即一个用户线程对应一个内核线程

![内核线程模型](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/21-%E7%BA%BF%E7%A8%8BPCB-%E4%B8%80%E5%AF%B9%E4%B8%80%E5%85%B3%E7%B3%BB.jpg)

内核的优缺点就和用户线程的恰恰相反。

优点：

1. 分配给线程、多线程的进程获得更多的 CPU 运行时间
2. 在一个进程当中，如果某个内核线程发起系统调用而被阻塞，并不会影响其他内核线程的运行；

缺点：

1. 线程的创建、终止和切换都是通过系统调用的方式来进行，因此对于系统来说，系统开销比较大；

#### 轻量级进程

轻量级进程（Light-weight process，LWP）是内核支持的用户线程，一个进程可有一个或多个 LWP，每个 LWP 是跟内核线程一对一映射的，也就是 LWP 都是由一个内核线程支持，而且 LWP 是由内核管理并像普通进程一样被调度。

在 LWP 之上也是可以使用用户线程的，且 LWP 和用户线程的对应关系也有一对一、一对多、多对一三种。再结合它和内核线程的一对一，就是这样的关系：

![LWP 模型](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/22-LWP.jpg)

## 进程和线程的区别

- 调度：进程是资源管理的基本单位，线程是程序执行的基本单位。
- 切换：线程上下文切换比进程上下文切换要快得多，创建、删除等操作也快得多。
- 拥有资源： 进程是拥有资源的一个独立单位，线程不拥有系统资源，但是可以访问隶属于进程的资源。
- 系统开销： 创建或撤销进程时，系统都要为之分配或回收系统资源，如内存空间，I/O 设备等，OS 所付出的开销显著大于在创建或撤销线程时的开销，进程切换的开销也远大于线程切换的开销。
- 状态和调度：线程的状态以及调度方式和进程基本相同，进程的同步方式也适用于线程

## I/O密集型和计算密集型（CPU密集型）

- CPU密集型，也叫计算密集型，一般是指服务器的硬盘、内存硬件性能相对CPU好很多，或者使用率低很多。系统运行CPU读写I/O(硬盘/内存)时可以在很短的时间内完成，几乎没有阻塞（等待I/O的实时间）时间，而CPU一直有大量运算要处理，因此CPU负载长期过高。

当CPU需要很长时间占用，那么进行频繁的线程切换就没有意义，因为大部分时间是在等待CPU的计算。在这种情况下，我们不需要为每个进程创建很多线程，通常一个进程保留一个线程即可，或者等于cpu的核数

一般其计算公式可遵循：CPU密集型核心线程数 = CPU核数。

常见的CPU密集型操作有：图像处理、加解密、编译、跑机器学习等

- I/O密集型相反，一般是指服务器CPU的性能相对硬盘、内存硬件好很多，或者使用率低很多。系统运行多是CPU在等I/O (硬盘/内存) 的读写操作，此类情景下CPU负载并不高。

  I/O密集型的程序一般在达到性能极限时，CPU占用率仍然较低。这可能是因为任务本身需要大量I/O操作，而程序的逻辑做得并不好，没有充分利用CPU能力，导致线程空余时间很多。通常我们会开CPU核心数数倍的线程，在线程进行 I/O 操作 CPU 空闲时，启用其他线程继续使用 CPU，以提高 CPU 的使用率，充分利用CPU资源。

一般其计算公式可遵循：I/O密集型核心线程数 = CPU核数 / （1-阻塞系数）。

常见的IO密集型任务有：文件操作、网络通信、数据库读写等

# 死锁问题

## 死锁的定义

死锁（Deadlock），是指多个进程在运行过程中因争夺资源而造成的一种僵局，当进程处于这种僵持状态时，若无外力作用，它们都将无法再向前推进。
通俗来说，死锁出现于采用互斥访问方法、不可以被抢占的资源，即临界资源。
一个进程占用一部分资源并请求下一个资源，但是其中的每个进程都保持这种状态，每个进程都要请求被另外一个进程正在占用的资源，并且没办法放掉自己现在占用的资源，就会造成死锁。即这组进程的**每个进程都在等待另一个死锁进程占有的资源。**

## 产生死锁的必要条件

1. 互斥：进程需要的每个资源都只能同时被一个进程访问，进程都是互斥的。
2. 请求和保持：每个进程自己占有一个资源，又请求下一个资源，并且进程请求下一个资源时并不会释放上一个占用的资源
3. 不可抢占：即进程占用的资源在自己主动释放之前不能被任何手段抢占
4. 循环等待：发生死锁时，必然有一个循环的等待，比如 A 等 B，B 等 C，C 又等待 A，形成环路。

## 预防死锁的方法

核心就是破坏产生死锁的四个必要条件中的一个。具体来说是第 2、3、4 个；必要条件 1，因为它是由硬件设备的固有属性所决定的，不仅不能改变，还应加以保证。

1. 破坏“请求和保持”条件：规定所有进程在开始运行之前，都必须**一次性申请其在整个运行过程所需的全部资源**。
2. 摒弃“不可抢占”条件： 当一个已经保持了某些资源的进程，再提出新的资源请求而不能立即得到满足时，必须**释放它已经保持了的所有资源**。待以后需要时再重新申请。
3. 摒弃“环路等待”条件：即通过调整合适的进程执行顺序，防止出现环路等待；这个方法就是下面说到的，找到一个“安全序列”

## 避免死锁的方法

避免死锁可以通过一定的算法来解决，核心是保证系统运行过程中的安全状态。

所谓安全状态，是指系统能按某种进程顺序，如`<P1， P2，…，Pn>`，依次为 n 个进程分配其所需资源，直至其最大需求，使每个进程都可顺利地完成，称系统处于安全状态，并称`<P1，P2，…，Pn>`序列为安全序列。否则，如果系统无法找到这样一个安全序列，则称系统处于不安全状态。

系统决定能不能将资源分配给某个进程的依据，就是将资源分配给该进程后，该进程可以执行完成并释放出本来已经占有的资源数量，释放之后（原本就有的+该进程释放出来的）的资源总数仍然能够下一个进程使用。依次对每个进程都进行这样的操作，就可以得到一个安全序列

比如现在系统有三个进程 P1、P2 和 P3，共有 14 台打印机。进程的各需求情况如下：
![](https://pic.imgdb.cn/item/627dd80b0947543129ccaa9b.jpg)

现在可用 3 台，因此可以：

1. 先把 2 台给 P2，P2 完成后返回占用的 2 台，可用 5 台；
2. 把 5 台分给 P3，P3 完成后返回占用的 5 台，可用 10 台
3. 剩下的分给 P1

因此 T0 时刻系统是安全的, 存在一个安全序列`<P2,P3,P1>`

相反，如果在某一时刻分配给任何一个进程都不能满足一个进程的需要，并且分配之后的可用也不可以，就会陷入死锁，这时就是不安全状态。

---

避免死锁的一个著名算法是银行家算法，其核心就是保证每次分配资源时都能找到一个安全序列。


# 内存管理

## 虚拟内存

### 为什么存在虚拟内存

把进程所使用的地址「隔离」开来，即让操作系统为每个进程分配独立的一套「虚拟地址」。每个进程只需要确定自己需要访问的虚拟地址，至于虚拟地址和物理地址的对应则由操作系统完成。

需要虚拟内存的主要原因在于，如果进程访问的都是直接的物理内存，那么两个进程就不能访问一个地址，否则不能并行。对于几十、几百个进程甚至更多，物理地址的选择就会成很大问题。

- 我们程序所使用的内存地址叫做虚拟内存地址（Virtual Memory Address）
- 实际存在硬件里面的空间地址叫物理内存地址（Physical Memory Address）

### 虚拟内存的作用

虚拟内存有什么作用？

- 第一，虚拟内存可以使得进程对运行内存超过物理内存大小，因为程序运行符合局部性原理，CPU 访问内存会有很明显的重复访问的倾向性，对于那些没有被经常使用到的内存，我们可以把它换出到物理内存之外，比如硬盘上的 swap 区域。
- 第二，由于每个进程都有自己的页表，所以每个进程的虚拟内存空间就是相互独立的。进程也没有办法访问其他进程的页表，所以这些页表是私有的，这就解决了多进程之间地址冲突的问题。
- 第三，页表里的页表项中除了物理地址之外，还有一些标记属性的比特，比如控制一个页的读写权限，标记该页是否存在等。在内存访问方面，操作系统提供了更好的安全性。

虚拟内存可以极大的扩展实际内存的大小。对于一个进程来说，即使没有那么多的物理内存，也可以申请到比实际物理内存大的多的虚拟内存。比如在 32 位操作系统进程理论上最大能申请 3 GB 大小的虚拟内存，而 64 位操作系统进程理论上最大能申请 128 TB 大小的虚拟内存，即使物理内存只有 4GB，申请 8G 内存也是没问题，因为申请的内存是虚拟内存。
只有当虚拟内存被访问时，操作系统才会执行换出换入、部分无用内存回收、OOM 等机制来满足物理内存的需求。

### 虚拟内存的管理方式

管理虚拟内存的方式主要有两种：

- 内存分段：程序是由若干个逻辑分段组成的，如可由代码分段、数据分段、栈段、堆段组成。不同的段是有不同的属性的，所以就用分段（Segmentation）的形式把这些段分离出来。
- 内存分页

#### 分段

分段机制下的虚拟地址由两部分组成，段选择因子和段内偏移量。

![](https://pic.imgdb.cn/item/641d9285a682492fccf0ae9c.jpg)

如上图，分段内存管理的关键内容如上：

- 段：对应一个物理内存中的段。每个段有自己的起始地址（段基址）和段的边界（段界限）。
- 段表：用于记录段基址、段边界等和段号之间的对应关系。通过段号查询到对应的段基址，加上偏移量就可以得到对应的实际物理地址
- 段号：用于查询段表的索引号
- 段内偏移：顾名思义，表示该段在物理地址中的偏移量。段内偏移量应该位于 0 和段界限之间，如果段内偏移量是合法的，就将段基地址加上段内偏移量得到物理内存地址。

虚拟地址是通过段表与物理地址进行映射的，分段机制会把程序的虚拟地址分成 4 个段，每个段在段表中有一个项，在这一项找到段的基地址，再加上偏移量，于是就能找到物理内存中的地址
比如一个程序通常会有代码段、数据段、堆栈段等。

![](https://pic.imgdb.cn/item/641d93caa682492fccf2cd10.jpg)

---

分段内存管理的问题：

1. 内存碎片化：程序分段不一定是连续的，每个段之间可能会有一些没有使用的地址，但是这段空白地址没办法给其他程序的其他段使用。

内存碎片主要分为，内部内存碎片和外部内存碎片。

> 内部碎片是已经被分配出去的的内存空间大于请求所需的内存空间。 内部碎片常出现于分页管理中。为了有效的利用内存，使内存产生更少的碎片，要对内存分页，内存以页为单位来使用，最后一页往往装不满，于是形成了内部碎片。  
> 外部碎片是指还没有分配出去，但是由于大小太小而无法分配给申请空间的新进程的内存空间空闲块外部碎片存在于分段管理中，如下所述；

内存分段管理可以做到段根据实际需求分配内存，所以有多少需求就分配多大的段，所以不会出现内部内存碎片。
但是由于每个段的长度不固定，所以多个段未必能恰好使用所有的内存空间，会产生了多个不连续的小物理内存，导致新的程序无法被装载，所以会出现外部内存碎片的问题。
解决「外部内存碎片」的问题就是内存交换。

可以把音乐程序占用的那 256MB 内存写到硬盘上，然后再从硬盘上读回来到内存里。不过再读回的时候，我们不能装载回原来的位置，而是紧紧跟着那已经被占用了的 512MB 内存后面。这样就能空缺出连续的 256MB 空间，于是新的 200MB 程序就可以装载进来。（类似 js 垃圾回收中的新生代区域机制，即整理内存碎片）

2. 内存交换效率低：

效率低的主要原因可以理解为需要频繁整理内存，但整理内存的过程比较慢。
对于多进程的系统来说，用分段的方式，外部内存碎片是很容易产生的，产生了外部内存碎片，那不得不重新 Swap 内存区域，这个过程会产生性能瓶颈。因为硬盘的访问速度要比内存慢太多了，每一次内存交换，我们都需要把一大段连续的内存数据写到硬盘上。
所以，如果内存交换的时候，交换的是一个占内存空间很大的程序，这样整个机器都会显得卡顿。

#### 分页

为了解决内存分段的「外部内存碎片和内存交换效率低」的问题，就出现了内存分页。
分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小。这样一个连续并且尺寸固定的内存空间，我们叫页（Page）。在 Linux 下，每一页的大小为 4KB。

![](https://pic.imgdb.cn/item/641d9886a682492fccfabc20.jpg)

页表是存储在内存里的，内存管理单元 （MMU）就做将虚拟内存地址转换成物理地址的工作。
而当进程访问的虚拟地址在页表中查不到时，系统会产生一个缺页异常，进入系统内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行。

内存分页由于内存空间都是预先划分好的，也就不会像内存分段一样，在段与段之间会产生间隙非常小的内存，这正是分段会产生外部内存碎片的原因。而采用了分页，**页与页之间是紧密排列**的，所以不会有外部碎片。
但是，因为内存分页机制分配内存的最小单位是一页，即使程序不足一页大小，我们最少只能分配一个页，所以页内会出现内存浪费，所以针对内存分页机制会有**内部内存碎片**的现象。

如果内存空间不够，操作系统会把其他正在运行的进程中的「最近没被使用」的内存页面给释放掉，也就是暂时写在硬盘上，称为换出（Swap Out）。一旦需要的时候，再加载进来，称为换入（Swap In）。所以，一次性写入磁盘的也只有少数的一个页或者几个页，不会花太多时间，内存交换的效率就相对比较高。

甚至完全可以在进行虚拟内存和物理内存的页之间的映射之后（即相当于初始化阶段），并不真的把页从硬盘换入到物理内存里，而是只有在程序运行中，需要用到对应虚拟内存页里面的指令和数据时，再加载到物理内存里面去。

在分页机制下，虚拟地址分为两部分，页号和页内偏移。页号作为页表的索引，页表包含物理页每页所在物理内存的基地址，这个基地址与页内偏移的组合就形成了物理内存地址。
这个对应机制和分段差不多，都是通过一个序号在表中查询。不过页表中存储的不是页的物理地址，而是虚拟页号和物理页号的映射。

![](https://pic.imgdb.cn/item/641d9965a682492fccfc886d.jpg)

但是这样就有一个问题，页表会非常庞大。因为页表的每一项只对应一个物理页号，如果需要非常多的页，那么页表也会非常大，否则就需要很频繁地调页。
要解决上面的问题，就需要采用一种叫作多级页表（Multi-Level Page Table）的解决方案。

对于单页表的实现方式，在 32 位和页大小 4KB 的环境下，一个进程的页表需要装下 100 多万个「页表项」，并且每个页表项是占用 4 字节大小的，于是相当于每个页表需占用 4MB 大小的空间。
我们把这个 100 多万个「页表项」的单级页表再分页，将页表（一级页表）分为 1024 个页表（二级页表），每个表（二级页表）中包含 1024 个「页表项」，形成二级分页。

一级页表内，页号和二级页表地址对应，即从虚拟页号对应到二级页表中的页号；二级页表内则是由页号对应到物理页号。

> 分了二级表，映射 4GB 地址空间就需要 4KB（一级页表）+ 4MB（二级页表）的内存，这样占用空间不是更大了吗？
> 实际上不会，因为一个进程不会真的需要那么多内存。如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了，即可以在需要时才创建二级页表。
> 比如，只有 20% 的一级页表项被用到了，那么页表占用的内存空间就只有 4KB（一级页表） + 20% \* 4MB（二级页表）= 0.804MB
> 而对于一级页表，假如虚拟地址在页表中找不到对应的页表项，计算机系统就不能工作了。所以页表一定要覆盖全部虚拟地址空间，不分级的页表就需要有 100 多万个页表项来映射，而二级分页则只需要 1024 个页表项

更高级的操作系统就需要更多的页表级别，比如 64 位计算机的页表有四级。

#### TLB

多级页表虽然解决了空间上的问题，但是虚拟地址到物理地址的转换就多了几道转换的工序，这显然就降低了这俩地址转换的速度，也就是带来了时间上的开销。

程序是有局部性的，即在一段时间内，整个程序的执行仅限于程序中的某一部分。相应地，执行所访问的存储空间也局限于某个内存区域。可以利用这一特性，把最常访问的几个页表项存储到访问速度更快的硬件中缓存起来，每次查表之前现在 TLB 中查找，如果没找到再去查页表。
这个 Cache 就是 TLB（Translation Lookaside Buffer） ，通常称为页表缓存、转址旁路缓存、**快表**等。
TLB 的命中率其实是很高的，因为程序最常访问的页就那么几个

![](https://pic.imgdb.cn/item/641d9c0ea682492fcc0259c1.jpg)

#### 段页式内存管理

段页式内存管理实现的方式：

- 先将程序划分为多个有逻辑意义的段，也就是前面提到的分段机制；
- 接着再把每个段划分为多个页，也就是对分段划分出来的连续空间，再划分固定大小的页；

这样，地址结构就由段号、段内页号和页内位移三部分组成。用于段页式地址变换的数据结构是每一个程序一张段表，每个段又建立一张页表，段表中的地址是页表的起始地址，而页表中的地址则为某页的物理页号

![](https://pic.imgdb.cn/item/641d9cc6a682492fcc03762d.jpg)

段页式地址变换中要得到物理地址须经过三次内存访问：

- 第一次访问段表，得到页表起始地址；（先通过段访问到存储段的地方）
- 第二次访问页表，得到物理页号；（在段内通过分页存储查找）
- 第三次将物理页号与页内位移组合，得到物理地址。

### 内存的换入换出

程序申请的虚拟内存，如果没有被使用，它是不会占用物理空间的。当访问这块虚拟内存后，操作系统才会进行物理内存分配。

如果申请物理内存大小超过了空闲物理内存大小，就要看操作系统有没有开启 Swap 机制：

- 如果没有开启 Swap 机制，程序就会直接 OOM；（OOM 可以理解为，杀死一些进程以释放内存）
- 如果有开启 Swap 机制，程序可以正常运行。

当系统的物理内存不够用的时候，就需要将物理内存中的一部分空间释放出来，以供当前运行的程序使用。那些被释放的空间可能来自一些很长时间没有什么操作的程序，这些被释放的空间会被临时保存到磁盘，等到那些程序要运行时，再从磁盘中恢复保存的数据到内存中。

另外，当内存使用存在压力的时候，会开始触发内存回收行为，会把这些不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了。

Swap 就是把一块磁盘空间或者本地文件，当成内存来使用，它包含换出和换入两个过程：

- 换出（Swap Out） ，是把进程暂时不用的内存数据存储到磁盘中，并释放这些数据占用的内存；
- 换入（Swap In），是在进程再次访问这些内存的时候，把它们从磁盘读到内存中来；

使用 Swap 机制优点是，应用程序实际可以使用的内存空间将远远超过系统的物理内存。

## 页面置换算法

当 CPU 访问的页面不在物理内存时，便会产生一个缺页中断，请求操作系统将所缺页调入到物理内存。那它与一般中断的主要区别在于：

- 缺页中断在指令执行「期间」产生和处理中断信号，而一般中断在一条指令执行「完成」后检查和处理中断信号。
- 缺页中断返回到该指令的开始重新执行「该指令」，而一般中断返回回到该指令的「下一个指令」执行。

流程如下：

![](https://pic.imgdb.cn/item/641eb5eaa682492fcc9e6a4d.jpg)

1. 在 CPU 里访问一条 Load M 指令，然后 CPU 会去找 M 所对应的页表项。
1. 如果该页表项的状态位是「有效的」，那 CPU 就可以直接去访问物理内存了，如果状态位是「无效的」，则 CPU 则会发送缺页中断请求。
1. 操作系统收到了缺页中断，则会执行缺页中断处理函数，先会查找该页面在磁盘中的页面的位置。
1. 找到磁盘中对应的页面后，需要把该页面换入到物理内存中，但是在换入前，需要在物理内存中找空闲页，如果找到空闲页，就把页面换入到物理内存中。
1. 页面从磁盘换入到物理内存完成后，则把页表项中的状态位修改为「有效的」。
1. 最后，CPU 重新执行导致缺页异常的指令。

上面的第 4 步会检查是否存在空闲页。如果物理内存中已经没有空闲页了，那就需要通过页面置换算法来把一个物理页换出，再把需要换入的页插入。
页面置换算法的功能是，当出现缺页异常，需调入新页面而内存已满时，选择被置换的物理页面。也就是说选择一个物理页面换出到磁盘，然后把需要访问的页面换入到物理页。

常见的页面置换算法有如下几种：

- 最佳页面置换算法（OPT）
- 先进先出置换算法（FIFO）
- 最近最久未使用的置换算法（LRU）
- 时钟页面置换算法（Lock）
- 最不常用置换算法（LFU）

#### 最佳页面置换算法

![](https://pic.imgdb.cn/item/641eb6b6a682492fcc9fa8c7.jpg)

该算法实现需要计算内存中每个逻辑页面的「下一次」访问时间，然后比较，选择未来最长时间不访问的页面。实际系统中无法实现，因为程序访问页面时是动态的，我们是无法预知每个页面在「下一次」访问前的等待时间。
最佳页面置换算法作用是为了衡量你的算法的效率，你的算法效率越接近该算法的效率，那么说明你的算法是高效的

#### 先进先出置换算法

![](https://pic.imgdb.cn/item/641eb6eaa682492fcca0001a.jpg)

选择在内存驻留时间很长的页面进行中置换，即每次准备换入一个页时，检查现在物理内存中保存最长时间的那个页，将其换为新的页。

#### 最近最久未使用的置换算法

即 LRU 算法，这个很熟悉了。

![](https://pic.imgdb.cn/item/641eb764a682492fcca0d6b3.jpg)

虽然 LRU 在理论上是可以实现的，但代价很高。为了完全实现 LRU，需要在内存中维护一个所有页面的链表，最近最多使用的页面在表头，最近最少使用的页面在表尾。

困难的是，在每次访问内存时都必须要更新「整个链表」。在链表中找到一个页面，删除它，然后把它移动到表头是一个非常费时的操作。

所以，LRU 虽然看上去不错，但是由于开销比较大，实际应用中比较少使用。

#### 时钟页面置换算法

该算法的思路是，把所有的页面都保存在一个类似钟面的「环形链表」中，一个表针指向最老的页面。

当发生缺页中断时，算法首先检查表针指向的页面：

- 如果它的访问位是 0 就淘汰该页面，并把新的页面插入这个位置，然后把表针前移一个位置；
- 如果访问位是 1 就清除访问位（将访问位设为 0），并把表针前移一个位置，重复这个过程直到找到了一个访问位为 0 的页面为止；

![](https://pic.imgdb.cn/item/641eb7b4a682492fcca173dd.jpg)

## 磁盘调度算法

参考https://xiaolincoding.com/os/5_schedule/schedule.html#%E7%A3%81%E7%9B%98%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95

磁盘调度算法主要是针对磁盘信道访问的顺序。即这种形式：

假设有下面一个请求序列，每个数字代表磁道的位置：

98，183，37，122，14，124，65，67

初始磁头当前的位置是在第 53 磁道。

接下来，分别对以上的序列，作为每个调度算法的例子，那常见的磁盘调度算法有：

- 先来先服务算法
- 最短寻道时间优先算法
- 扫描算法
- 循环扫描算法
- LOOK 与 C-LOOK 算法

磁盘调度算法基本的思路就是按照一定顺序移动磁针，使得磁盘能在最短时间内访问到指定序列里的所有磁道。

# 计组相关

## CPU

### 冯诺依曼模型

计算机基本结构为 5 个部分，分别是**运算器、控制器、存储器、输入设备、输出设备**

现代计算机是在该模型的基础上发展的，主要有以下几个核心部分：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E7%A8%8B%E5%BA%8F%E6%89%A7%E8%A1%8C/%E5%86%AF%E8%AF%BA%E4%BE%9D%E6%9B%BC%E6%A8%A1%E5%9E%8B.png)

1. 内存：程序和数据都是存储在内存，存储的区域是线性的。存储数据的基本单位是字节。内存的地址是从 0 开始编号的，然后自增排列，最后一个地址为内存总字节数 - 1

2. CPU：有 32 位和 64 位之分，区别在于一次能计算多少字节数据。32 位计算机一次能计算 4 个字节（4\*8=32），64 位则可以计算 8 个字节。位数越大，计算机单次能表示的数越大（32 位的最大值是 2^32^，64 位则是 2^64^），计算的次数就会减少，效率也会提高。

   软件也有 32 位和 64 位之分，对于软件来说位数表示指令的位宽，因此 32 位的软件可以运行在 64 位电脑上，反过来就不行了

   1. 寄存器：存储计算时的数据，由于内存相对较远，一些临时数据没必要存在内存中，就可以存在寄存器中。常用的寄存器有程序计数器、指令寄存器等
   2. 控制单元：控制 CPU
   3. 逻辑运算单元：比如加法器等，用于进行计算

3. 总线：用于 CPU 和内存以及其他设备之间的通信，常用的有三种：

   1. _地址总线_，用于指定 CPU 将要操作的内存地址；
   2. _数据总线_，用于读写内存的数据；
   3. _控制总线_，用于发送和接收信号，比如中断、设备复位等信号，CPU 收到信号后自然进行响应，这时也需要控制总线；

4. 输入输出设备

### 程序执行的过程

程序实际上是一条一条指令，所以程序的运行过程就是把每一条指令一步一步的执行起来，负责执行指令的就是 CPU 了

![](https://pic.imgdb.cn/item/641ec5aba682492fccbd0bac.jpg)

CPU 执行程序的过程如下：

- 第一步，CPU 读取「程序计数器」的值，这个值是指令的内存地址，然后 CPU 的「控制单元」操作「地址总线」指定需要访问的内存地址，接着通知内存设备准备数据，数据准备好后通过「数据总线」将指令数据传给 CPU，CPU 收到内存传来的数据后，将这个指令数据存入到「指令寄存器」。
- 第二步，「程序计数器」的值自增，表示指向下一条指令。这个自增的大小，由 CPU 的位宽决定，比如 32 位的 CPU，指令是 4 个字节，需要 4 个内存地址存放，因此「程序计数器」的值会自增 4；
- 第三步，CPU 分析「指令寄存器」中的指令，确定指令的类型和参数，如果是计算类型的指令，就把指令交给「逻辑运算单元」运算；如果是存储类型的指令，则交由「控制单元」执行；

## 数的表示

### 补码

负数要用补码的形式表示，主要是为了保证加减法的有效性。
即，如果不使用补码，计算之前还需要判断数是正数还是负数，这个过程非常麻烦。

补码的计算：
- 如果是负数，原码全部取反，然后末位+1，符号位为1
- 如果是正数，那么不用改变。正数的原码、补码、反码相同

举个例子：
```
假如不用补码
1表示为0 0001
-2表示为1 0010
那么-2 + 1 = 0 0001 + 1 0010 = 1 0011 = -3
因为这里有一个数是负数，正常情况下应该使用减运算才对

如果使用补码：
1表示为0 0001
-2表示为1 1101
那么-2 + 1 = 0 0001 + 1 1101 = 1 1110，转为原码后为0001，即1 0001 = -1
这样就对了
```

### 浮点数

计算机中数字的表示方式是浮点数，通常有 32 位和 64 位两种。

浮点数采用的记录方式和科学计数法类似；比如十进制数`12345`用科学计数法表示为 $1.2345 \times 10^4 $，二进制数也可以这么表示。我们把这么规范化后的小数点后的部分称为尾数，n 次方称为指数，再加上符号位，就可以得到计算机表示浮点数的方式：

![img](https://blog-img-1307852525.cos.ap-chengdu.myqcloud.com/img/float.png)

浮点数有三个部分：

- 符号位：表示数字是正数还是负数，为 0 表示正数，为 1 表示负数；
- 指数位：指定了小数点在数据中的位置，指数可以是负数，也可以是正数，指数位的长度越长则数值的表达范围就越大；
- 尾数位：小数点右侧的数字，也就是小数部分，比如二进制 1.0011 x 2^(-2)，尾数部分就是 0011，而且尾数的长度决定了这个数的精度，因此如果要表示精度更高的小数，则就要提高尾数位的长度；

其中指数的表示是移码方式，即相当于`指数+ (2 ^ 指数位 - 1) `的值。比如 32 位的指数的值就是`实际指数+127`，64 位的则是+1023。
加上偏移量的目的主要是避免负指数的出现，因为指数位不好表示为负数。



注意这种形式只是计算机记录数值的形式。对于运算过程，可以理解为是把这种形式的数字转为正常的二进制数值再计算。

比如经典的 0.1+0.2!==0.3 问题，核心原因就是 0.1 和 0.2 转换为二进制小数（**乘 2 取整法**）会导致无限循环。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%B5%AE%E7%82%B9/%E6%97%A0%E9%99%90%E5%B0%8F%E6%95%B0.png)


## 中断

在计算机中，中断是系统用来响应硬件设备请求的一种机制，操作系统收到硬件的中断请求，会打断正在执行的进程，然后调用内核中的中断处理程序来响应请求。
中断是一种异步的事件处理机制，可以提高系统的并发处理能力。
操作系统收到了中断请求，会打断其他进程的运行，所以中断请求的响应程序，也就是中断处理程序，要尽可能快的执行完，这样可以减少对正常进程运行调度地影响。



# 数据库

## 数据库相关概念

- 数据库 : 数据库(DataBase 简称 DB)就是信息的集合或者说数据库是由数据库管理系统管理的数据的集合。
- 数据库管理系统 : 数据库管理系统(Database Management System 简称 DBMS)是一种操纵和管理数据库的大型软件，通常用于建立、使用和维护数据库。
- 数据库系统 : 数据库系统(Data Base System，简称 DBS)通常由软件、数据库和数据管理员(DBA)组成。
- 数据库管理员 : 数据库管理员(Database Administrator, 简称 DBA)负责全面管理和控制数据库系统。
- 元组 ： 元组（tuple）是关系数据库中的基本概念，关系是一张表，表中的每行（即数据库中的每条记录）就是一个元组，每列就是一个属性。 在二维表里，元组也称为行。
键 ：又叫码，码就是能唯一标识实体的属性，对应表中的列。
- 候选键 ： 若关系中的某一属性或属性组的值能唯一的标识一个元组，而其任何、子集都不能再标识，则称该属性组为候选码。例如：在学生实体中，“学号”是能唯一的区分学生实体的，同时又假设“姓名”、“班级”的属性组合足以区分学生实体，那么{学号}和{姓名，班级}都是候选码。
- 主键 : 主键也叫主码。主码是从候选码中选出来的。 一个实体集中只能有一个主码，但可以有多个候选码。
- 外键 : 外键也叫外码。如果一个关系中的一个属性是另外一个关系中的主码则这个属性为外码。
- 主属性 ： 即可以作为主键的，可以唯一标识一个元素的属性。候选码中出现过的属性称为主属性。比如关系 工人（工号，身份证号，姓名，性别，部门）. 显然工号和身份证号都能够唯一标示这个关系，所以都是候选码。工号、身份证号这两个属性就是主属性。如果主码是一个属性组，那么属性组中的属性都是主属性。
- 非主属性： 不包含在任何一个候选码中的属性称为非主属性。比如在关系——学生（学号，姓名，年龄，性别，班级）中，主码是“学号”，那么其他的“姓名”、“年龄”、“性别”、“班级”就都可以称为非主属性。

## sql语句

三个删除操作的语句：

- drop(丢弃数据): drop table 表名 ，直接将表都删除掉，在删除表的时候使用。
- truncate (清空数据) : truncate table 表名 ，只删除表中的数据，再插入数据的时候自增长 id 又从 1 开始，在清空表中数据的时候使用。
- delete（删除数据） : delete from 表名 where 列名=值，删除某一行的数据，如果不加 where 子句和truncate table 表名作用类似。

sql语句分为几个大的类型：

1. 数据定义语言（DDL）

数据定义语言（Data Definition Language，DDL）是 SQL 语言集中负责数据结构定义与数据库对象定义的语言。
DDL 的主要功能是定义数据库对象。
DDL 的核心指令是 CREATE、ALTER、DROP。

2. 数据操纵语言（DML）

数据操纵语言（Data Manipulation Language, DML）是用于数据库操作，对数据库其中的对象和数据运行访问工作的编程语句。
DML 的主要功能是 访问数据，因此其语法都是以读写数据库为主。
DML 的核心指令是 INSERT、UPDATE、DELETE、SELECT。这四个指令合称 CRUD(Create, Read, Update, Delete)，即增删改查。

3. 事务控制语言（TCL）

事务控制语言 (Transaction Control Language, TCL) 用于管理数据库中的事务。这些用于管理由 DML 语句所做的更改。它还允许将语句分组为逻辑事务。
TCL 的核心指令是 COMMIT、ROLLBACK。

4. 数据控制语言（DCL）

数据控制语言 (Data Control Language, DCL) 是一种可对数据访问权进行控制的指令，它可以控制特定用户账户对数据表、查看表、预存程序、用户自定义函数等数据库对象的控制权。
DCL 的核心指令是 GRANT、REVOKE。
DCL 以控制用户的访问权限为主，因此其指令作法并不复杂，可利用 DCL 控制的权限有：CONNECT、SELECT、INSERT、UPDATE、DELETE、EXECUTE、USAGE、REFERENCES。
根据不同的 DBMS 以及不同的安全性实体，其支持的权限控制也有所不同。

## 索引

索引的定义就是帮助存储引擎快速获取数据的一种数据结构，形象的说就是索引是数据的目录。
索引可以看作是一张表，其中包含了数据库表中某些列的值以及这些值对应的行的位置。通过使用索引，数据库可以更快速地定位到符合查询条件的数据行，从而加快查询速度。

假设我们有一个学生表 students，其中包含以下字段：id、name、age、gender、class_id。现在我们需要查询年龄为 20 岁的女生，查询语句如下：

```sql
SELECT * FROM students WHERE age = 20 AND gender = 'female'
```

如果我们没有为 age 和 gender 字段创建索引，那么数据库需要扫描整个表来查找符合条件的记录，查询效率会比较低。而如果我们为 age 和 gender 字段创建联合索引，相当于数据库就会根据gender和age来确定对应的行。即，数据库会查找那个age=20且gender=female的那一行，而这一行由于被标识为age=20且gender=female，因此查询速度很快，接近O(1)。数据库可以利用联合索引直接定位到符合条件的记录，而不需要扫描整个表，大大提高查询效率。

索引的列的值如果越互不相同，那么索引效率越高。因为不同列之间的数据相当于起到了不同的标识作用。如果记录的列存在大量相同的值，比如gender字段一半是female，另一半是male，那么相当于每次查找还需要从很多male、female中查找，效率就会很低。



